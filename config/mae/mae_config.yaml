# ===================================================
# MAE (Masked Autoencoder) Pre-training Configuration
# ===================================================

# === Data Configuration ===
data:
  train_path: "~/meghome/xec-ml-wl/data/E52.8_AngUni_PosSQ/large_train.root"
  val_path: "~/meghome/xec-ml-wl/data/E52.8_AngUni_PosSQ/large_val.root"
  tree_name: "tree"
  batch_size: 1024                  # Reduced from 8192 (MAE decoder needs more memory)
  chunksize: 256000
  num_workers: 1                    # Reduced for GH nodes (system suggested max=1)
  num_threads: 4                    # CPU preprocessing threads
  npho_branch: "npho"      # Input branch for photon counts (or "npho")
  time_branch: "relative_time"      # Input branch for timing (or "time")

# === Input Normalization ===
# Same as regressor training to ensure consistency
normalization:
  npho_scale: 1000
  npho_scale2: 4.08
  time_scale: 1.14e-7
  time_shift: -0.46
  sentinel_value: -1.0

# === Model Configuration ===
model:
  outer_mode: "finegrid"            # "finegrid" or "split"
  outer_fine_pool: [3, 3]           # null or [h, w] for pooling
  mask_ratio: 0.65                  # Fraction of input to mask (0.5-0.75 typical)
  time_mask_ratio_scale: 1.0        # Scale factor for masking valid-time sensors (>1.0 to prefer masking valid-time)

# === Training Configuration ===
training:
  epochs: 20
  lr: 2.5e-4
  lr_scheduler: null           # "cosine" or null
  lr_min: 1.0e-8
  warmup_epochs: 3
  weight_decay: 5.0e-5
  loss_fn: "smooth_l1"            # Options: smooth_l1, mse, l1, huber
  npho_weight: 1.0                # Per-channel manual weighting
  time_weight: 1.0                # Set <1.0 to downweight time channel
  auto_channel_weight: false      # Learn log(sigma^2) per channel (overrides manual weights)
  channel_dropout_rate: 0.0
  grad_clip: 1.0                    # Gradient clipping (0 to disable)
  grad_accum_steps: 1               # Gradient accumulation steps (effective batch = batch_size * grad_accum_steps)
  ema_decay: null                   # EMA decay rate (null to disable, 0.999 typical)
  amp: true                         # Automatic Mixed Precision
  compile: false                    # torch.compile (can cause LLVM errors with multiprocessing)
  npho_threshold: 100              # Npho threshold for conditional time loss (null uses default 10.0)
  use_npho_time_weight: true        # Weight time loss by sqrt(npho) for chi-square-like weighting
  track_mae_rmse: false             # Compute/log MAE/RMSE metrics (slower, set true for detailed analysis)
  track_train_metrics: false        # Track per-face loss during training (slower, set true for debugging)

# === Checkpointing ===
checkpoint:
  resume_from: null                 # Path to checkpoint to resume from
  save_dir: "artifacts"             # Directory to save checkpoints
  save_interval: 10                 # Save checkpoint every N epochs
  save_predictions: false            # Save sensor-level predictions to ROOT file

# === MLflow ===
mlflow:
  experiment: "mae_pretraining"
  run_name: null                    # null = auto-generate timestamp
