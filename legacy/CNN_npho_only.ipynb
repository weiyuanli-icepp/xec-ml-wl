{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc17047a-704f-4943-b7a4-82dd32deedb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, angle_model_geom\n",
    "importlib.reload(angle_model_geom)\n",
    "import importlib, train_angle\n",
    "importlib.reload(train_angle)\n",
    "import inspect, angle_model_geom\n",
    "# print(inspect.getsource(angle_model_geom.HexGraphConv.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "488f2d91-4d2a-4ec9-bd87-39b3d6876fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Storing artifacts to: artifacts/run_20251126_115621\n",
      "[info] using 2D (theta,phi) reweighting\n",
      "[001] train 54.676642  val 43.052794  time 104.7s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[002] train 36.882293  val 33.160866  time 105.2s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[003] train 32.516066  val 32.183643  time 103.9s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[004] train 31.985442  val 31.810105  time 105.4s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[005] train 31.610397  val 31.299521  time 104.1s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[006] train 31.126563  val 30.790809  time 104.5s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[007] train 30.422866  val 30.083955  time 102.4s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[008] train 29.599546  val 29.142045  time 105.0s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[009] train 28.727535  val 28.652537  time 104.7s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[010] train 28.028618  val 27.939467  time 103.3s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[011] train 27.444144  val 27.670010  time 102.8s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[012] train 27.133065  val 27.589448  time 106.4s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[013] train 26.868833  val 27.173507  time 102.4s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[014] train 26.719041  val 27.240337  time 119.1s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[015] train 26.607824  val 26.970239  time 112.4s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[016] train 26.540262  val 26.825498  time 114.7s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[017] train 26.473745  val 26.614169  time 110.3s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[018] train 26.404429  val 26.811873  time 110.8s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[019] train 26.341792  val 26.852139  time 106.8s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[020] train 26.166884  val 26.692858  time 113.6s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[021] train 26.228935  val 26.914096  time 106.7s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[022] train 26.085067  val 26.893793  time 103.4s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[023] train 26.099729  val 26.711499  time 112.4s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[024] train 26.055921  val 26.449224  time 107.9s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[025] train 26.085489  val 26.541624  time 103.1s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[026] train 26.006005  val 26.325718  time 114.8s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[027] train 26.006217  val 26.482021  time 104.5s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[028] train 25.959296  val 26.463517  time 103.8s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[029] train 25.895927  val 26.591347  time 104.1s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[030] train 25.871934  val 26.530752  time 103.5s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[031] train 25.878402  val 26.273655  time 106.9s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[032] train 25.765521  val 26.290688  time 111.0s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[033] train 25.764437  val 26.163540  time 108.6s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[034] train 25.791852  val 26.091394  time 102.7s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[035] train 25.721439  val 26.423313  time 106.8s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[036] train 25.674426  val 26.310723  time 104.4s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[037] train 25.680554  val 26.417342  time 102.3s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[038] train 25.735801  val 26.213724  time 103.9s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[039] train 25.632663  val 26.273834  time 113.4s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[040] train 25.672719  val 26.483725  time 104.9s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[041] train 25.628342  val 26.195722  time 106.7s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[042] train 25.621020  val 26.258108  time 110.1s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[043] train 25.605498  val 25.907494  time 108.0s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[044] train 25.529348  val 26.426391  time 104.4s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[045] train 25.565293  val 26.045637  time 105.5s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[046] train 25.542840  val 26.035476  time 102.3s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[047] train 25.473552  val 25.901742  time 115.4s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[048] train 25.517441  val 26.102809  time 104.9s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[049] train 25.419970  val 26.028601  time 105.6s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[050] train 25.452067  val 26.107292  time 105.7s\n",
      "   [mem] alloc=0.10 GB, peak=2.68 GB\n",
      "[info] best val_loss = 25.901742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/26 13:26:28 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Exported ONNX to meg2ang.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/26 13:26:28 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: TypeError('The PyTorch flavor does not support List or Dict input types. Please use a pandas.DataFrame or a numpy.ndarray'). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n",
      "2025/11/26 13:26:29 WARNING mlflow.utils.requirements_utils: Found torch version (2.4.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.4.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/11/26 13:28:06 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.19.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torchvision==0.19.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/11/26 13:28:07 WARNING mlflow.models.model: Failed to validate serving input example {\n",
      "  \"inputs\": {\n",
      "    \"Npho4760\": [\n",
      "      [\n",
      "        .... Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\n",
      "Got error: The PyTorch flavor does not support List or Dict input types. Please use a pandas.DataFrame or a numpy.ndarray\n"
     ]
    }
   ],
   "source": [
    "from train_angle import main_angle_with_args\n",
    "\"\"\"\n",
    "main_angle_with_args(\n",
    "    root=\"~/meghome/xec-ml-wl/data/MCGammaAngle_0-49.root\",\n",
    "    tree=\"tree\",\n",
    "    epochs=50,\n",
    "    chunksize=16000,\n",
    "    lr=3e-4,\n",
    "    amp=True,\n",
    "    npho_branch=\"relative_npho\",\n",
    "    NphoScale=2e5,\n",
    "    max_chunks=None,\n",
    "    mlflow_experiment=\"CNN_npho_only\",\n",
    "    run_name=\"third_try_geometry_modified\"\n",
    ")\n",
    "\"\"\"\n",
    "main_angle_with_args(\n",
    "    root=\"~/meghome/xec-ml-wl/data/MCGammaAngle_0-49.root\",\n",
    "    tree=\"tree\",\n",
    "    epochs=50,\n",
    "    batch=2024,\n",
    "    chunksize=32000,\n",
    "    lr=3e-4,\n",
    "    amp=True,\n",
    "    npho_branch=\"relative_npho\",\n",
    "    time_branch=\"relative_time\",\n",
    "    NphoScale=1.0,\n",
    "    reweight_mode=\"theta_phi\",\n",
    "    loss_type=\"smooth_l1\",\n",
    "    outer_mode=\"finegrid\",\n",
    "    outer_fine_pool=(3,3),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xec-ml-wl)",
   "language": "python",
   "name": "xec-ml-wl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
