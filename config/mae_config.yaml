# MAE (Masked Autoencoder) Pre-training Configuration
# ===================================================
# This config is for self-supervised pre-training of the encoder.
# The pre-trained weights can be used for fine-tuning the regressor.

# === Data Configuration ===
data:
  train_path: "/path/to/train"      # Directory or file pattern for training data
  val_path: "/path/to/val"          # Directory or file pattern for validation data (optional)
  tree_name: "tree"
  batch_size: 1024
  chunksize: 256000
  num_workers: 4                    # DataLoader workers

# === Input Normalization ===
# Same as regressor training to ensure consistency
normalization:
  npho_scale: 0.58
  npho_scale2: 1.0
  time_scale: 6.5e8
  time_shift: 0.5
  sentinel_value: -5.0

# === Model Configuration ===
model:
  outer_mode: "finegrid"            # "finegrid" or "split"
  outer_fine_pool: [3, 3]           # null or [h, w] for pooling
  mask_ratio: 0.6                   # Fraction of input to mask (0.5-0.75 typical)

# === Training Configuration ===
training:
  epochs: 20
  lr: 1.0e-4
  weight_decay: 1.0e-4
  channel_dropout_rate: 0.1

# === Checkpointing ===
checkpoint:
  resume_from: null                 # Path to checkpoint to resume from
  save_dir: "artifacts"             # Directory to save checkpoints

# === MLflow ===
mlflow:
  experiment: "mae_pretraining"
  run_name: null                    # null = auto-generate timestamp
