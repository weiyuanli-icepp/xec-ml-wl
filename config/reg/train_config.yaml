# ===========================
# XEC Regressor Configuration
# ===========================

# === Data Configuration ===
data:
  train_path: "~/meghome/xec-ml-wl/data/E52.8_AngUni_PosSQ/large_train.root"
  val_path: "~/meghome/xec-ml-wl/data/E52.8_AngUni_PosSQ/large_val.root"
  tree_name: "tree"
  batch_size: 4096
  chunksize: 256000
  num_workers: 8                    # DataLoader workers
  num_threads: 4                    # CPU preprocessing threads
  npho_branch: "npho"      # Input branch for photon counts (or "npho")
  time_branch: "relative_time"      # Input branch for timing (or "time")
  log_invalid_npho: true  # or false to disable

# === Input Normalization ===
normalization:
  npho_scale: 0.58
  npho_scale2: 1.0
  time_scale: 6.5e-8
  time_shift: 0.5
  sentinel_value: -5.0
  # Normalization scheme for npho:
  #   "log1p" (default), "anscombe", "sqrt", "linear"
  npho_scheme: "log1p"

# === Model Configuration ===
model:
  outer_mode: "finegrid"            # "finegrid" or "split"
  outer_fine_pool: null             # null or [h, w] for pooling
  hidden_dim: 256
  drop_path_rate: 0.0

# === Active Tasks ===
# Each task can have:
#   enabled: true/false
#   loss_fn: "smooth_l1", "l1", "mse", "huber"
#   loss_beta: float (for smooth_l1/huber)
#   weight: float (manual loss weight)
tasks:
  angle:
    enabled: true
    loss_fn: "smooth_l1"
    loss_beta: 1.0
    weight: 1.0
  energy:
    enabled: false
    loss_fn: "l1"
    loss_beta: 1.0
    weight: 1.0
  timing:
    enabled: false
    loss_fn: "l1"
    loss_beta: 1.0
    weight: 1.0
  uvwFI:
    enabled: false
    loss_fn: "mse"
    loss_beta: 1.0
    weight: 1.0

# === Training Configuration ===
training:
  epochs: 20
  lr: 3.0e-4
  weight_decay: 1.0e-4
  warmup_epochs: 2
  # Learning rate scheduler options: "cosine", "onecycle", "plateau", null (disable)
  # You can use either lr_scheduler (preferred) or legacy use_scheduler+scheduler
  lr_scheduler: "cosine"
  # Legacy scheduler config (use lr_scheduler instead):
  # use_scheduler: true
  # scheduler: "cosine"
  # Minimum LR (used by "cosine" and "plateau" schedulers)
  lr_min: 1.0e-6
  # OneCycleLR specific (only used when scheduler: "onecycle")
  # max_lr: 3.0e-4                  # Max LR (defaults to lr if not set)
  # pct_start: 0.3                  # Fraction of training for LR increase phase
  # ReduceLROnPlateau specific (only used when scheduler: "plateau")
  # lr_patience: 5                  # Epochs to wait before reducing LR
  # lr_factor: 0.5                  # Factor to reduce LR by
  amp: true
  ema_decay: 0.999
  channel_dropout_rate: 0.1
  grad_clip: 1.0
  # Gradient accumulation: effective_batch = batch_size Ã— grad_accum_steps
  # Use when GPU memory limits batch_size but larger effective batches improve convergence
  grad_accum_steps: 1
  profile: false                    # Enable training profiler to identify bottlenecks
  # torch.compile mode options:
  #   "max-autotune" - Maximum optimization, benchmarks many kernel configs (highest memory)
  #   "reduce-overhead" - Lower compilation overhead and memory, good balance
  #   "default" - Basic compilation with minimal overhead
  #   "false" or "none" - Disable compilation, use eager mode (no extra memory)
  compile: "reduce-overhead"

# === Loss Balancing ===
loss_balance: "manual"              # "manual" or "auto"

# === Reweighting ===
# Enable reweighting per task to balance sample distributions.
# Each task can have:
#   enabled: true/false
#   nbins: int (for 1D histograms)
#   nbins_2d: [int, int] (for 2D/3D histograms)
reweighting:
  angle:
    enabled: false
    nbins_2d: [20, 20]              # (theta_bins, phi_bins)
  energy:
    enabled: false
    nbins: 30
  timing:
    enabled: false
    nbins: 30
  uvwFI:
    enabled: false
    nbins_2d: [10, 10]              # Uses same bins for u, v, w

# === Checkpointing ===
checkpoint:
  resume_from: null                 # Path to checkpoint or null
  save_dir: "artifacts"
  save_interval: 10                 # Save checkpoint every N epochs (in addition to best/last)
  save_artifacts: true              # Save plots, CSVs, worst case displays (disable for quick testing)
  new_mlflow_run: false             # Create new MLflow run when resuming (vs continue existing)
  reset_epoch: false                # Reset epoch counter to 0 when resuming
  refresh_lr: false                 # Reset learning rate scheduler when resuming

# === Distributed Training ===
distributed:
  num_gpus: 1                       # Number of GPUs (1 = single-GPU, >1 = DDP multi-GPU)

# === MLflow ===
mlflow:
  experiment: "gamma_angle"
  run_name: null                    # null = auto-generate timestamp

# === Export ===
export:
  # ONNX filename only (not full path). File is saved to artifacts/<run_name>/.
  # Set to null to disable ONNX export.
  onnx: "meg2ang_convnextv2.onnx"
