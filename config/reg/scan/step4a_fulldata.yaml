# Step 4a full-data: Best config (step4a) trained on train_max with 4 GPUs
# Resume from step4a weights, retrain on full dataset

data:
  train_path: "~/meghome/xec-ml-wl/data/E15to60_AngUni_PosSQ/train_max"
  val_path: "~/meghome/xec-ml-wl/data/E15to60_AngUni_PosSQ/val"
  tree_name: "tree"
  batch_size: 1024
  chunksize: 131072
  num_workers: 2
  prefetch_factor: 16
  num_threads: 16
  npho_branch: "npho"
  time_branch: "relative_time"
  log_invalid_npho: true
  fiducial:
    enabled: true
    u_max: 23.9
    v_max: 67.9
    w_min: 0.0
    w_max: null

normalization:
  npho_scheme: sqrt
  npho_scale: 20000
  npho_scale2: 1.0
  time_scale: 1.14e-7
  time_shift: -0.46
  sentinel_time: -1.0
  sentinel_npho: -1.0

model:
  outer_mode: "finegrid"
  outer_fine_pool: [2, 2]
  hidden_dim: 256
  drop_path_rate: 0.1
  encoder_dim: 1024
  dim_feedforward: 4096
  num_fusion_layers: 2

tasks:
  angle:
    enabled: false
  energy:
    enabled: true
    loss_fn: "smooth_l1"
    loss_beta: 0.002
    weight: 1.0
    log_transform: false
  timing:
    enabled: false
  uvwFI:
    enabled: false

training:
  epochs: 50
  amp: true
  lr: 3.0e-4
  weight_decay: 1.0e-4
  lr_scheduler: "cosine"
  warmup_epochs: 5
  lr_min: 1.0e-6
  lr_patience: 5
  lr_factor: 0.5
  max_lr: null
  pct_start: 0.3
  ema_decay: 0.999
  channel_dropout_rate: 0
  grad_clip: 1.0
  grad_accum_steps: 1                  # effective batch = 1024*1*4gpu = 4096 (same as scan)
  profile: false
  compile: none
  compile_fullgraph: false

loss_balance: "manual"

reweighting:
  energy:
    enabled: false
    nbins: 30

checkpoint:
  resume_from: "~/meghome/xec-ml-wl/artifacts/scan_s4a_lr3e-4_resume/checkpoint_best.pth"
  save_dir: "artifacts"
  save_interval: 10
  save_artifacts: true
  new_mlflow_run: true
  reset_epoch: true
  refresh_lr: true

distributed:
  num_gpus: 4

mlflow:
  experiment: "gamma_energy"
  run_name: "scan_s4a_fulldata"

export:
  onnx: null
