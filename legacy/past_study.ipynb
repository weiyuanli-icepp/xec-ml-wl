{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3953aa7c-28d6-484a-a559-7ce380f5567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Streamed (memory-safe) trainer for MEG LXe energy regression.\n",
    "\n",
    "- Input: Npho (4760), split into SiPM(4092)→93x44 + PMT(668)\n",
    "- Preprocess: log1p(Npho / 2e5)\n",
    "- Model: CNN(SiPM) → flatten + PMT → FC(128) → FC(1)\n",
    "- Target: standardized z = (E - μ)/σ   (auto-computed)\n",
    "- Optional rebalance: \"loss\" | \"resample\"\n",
    "- Streaming: uproot.iterate (no full file in RAM)\n",
    "- ONNX export\n",
    "\"\"\"\n",
    "\n",
    "import os, re, time, argparse, warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import uproot\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  Utilities\n",
    "# ----------------------------------------------------------------------\n",
    "def iterate_chunks(path, tree, branches, step_size=4000):\n",
    "    \"\"\"Yield NumPy dicts (float64) chunk-by-chunk from ROOT TTree.\"\"\"\n",
    "    with uproot.open(path) as f:\n",
    "        t = f[tree]\n",
    "        for arrays in t.iterate(branches, step_size=step_size, library=\"np\"):\n",
    "            yield arrays\n",
    "\n",
    "\n",
    "def scan_energy_stats(root, tree, step_size=4000):\n",
    "    \"\"\"Single streamed pass to compute mean/std of 'energy'.\"\"\"\n",
    "    n, mean, M2 = 0, 0.0, 0.0\n",
    "    for arr in iterate_chunks(root, tree, [\"energy\"], step_size):\n",
    "        E = arr[\"energy\"].astype(\"float64\").ravel()\n",
    "        for x in E:\n",
    "            n += 1\n",
    "            delta = x - mean\n",
    "            mean += delta / n\n",
    "            M2 += delta * (x - mean)\n",
    "    if n < 2:\n",
    "        raise RuntimeError(\"Not enough samples for stats.\")\n",
    "    var = M2 / (n - 1)\n",
    "    return float(mean), float(np.sqrt(max(var, 1e-12)))\n",
    "\n",
    "\n",
    "def scan_energy_hist(root, tree, nbins=30, step_size=4000):\n",
    "    \"\"\"Streamed energy histogram for imbalance reweighting.\"\"\"\n",
    "    Emin, Emax = +np.inf, -np.inf\n",
    "    for arr in iterate_chunks(root, tree, [\"energy\"], step_size):\n",
    "        E = arr[\"energy\"].astype(\"float64\").ravel()\n",
    "        if E.size:\n",
    "            Emin, Emax = min(Emin, E.min()), max(Emax, E.max())\n",
    "    if not np.isfinite(Emin) or not np.isfinite(Emax):\n",
    "        raise RuntimeError(\"Invalid energy range.\")\n",
    "    edges = np.linspace(Emin, Emax, nbins + 1)\n",
    "    counts = np.zeros(nbins, np.int64)\n",
    "    for arr in iterate_chunks(root, tree, [\"energy\"], step_size):\n",
    "        E = arr[\"energy\"].astype(\"float64\").ravel()\n",
    "        if E.size:\n",
    "            h, _ = np.histogram(E, bins=edges)\n",
    "            counts += h\n",
    "    eps = 1e-9\n",
    "    inv = 1.0 / (counts + eps)\n",
    "    weights = inv / inv.mean()\n",
    "    return edges, counts, weights\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  Model\n",
    "# ----------------------------------------------------------------------\n",
    "class RegressorSimple(nn.Module):\n",
    "    def __init__(self, nlayer=16, nfc=128, drop_conv=0.3, drop_fc=0.0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, nlayer, (7,6), stride=(4,4), padding=(3,3))\n",
    "        self.conv2 = nn.Conv2d(nlayer, 2*nlayer, (4,4), stride=(2,2), padding=(1,1))\n",
    "        self.conv3 = nn.Conv2d(2*nlayer, 4*nlayer, (2,2), stride=(2,2))\n",
    "        self.bn1, self.bn2, self.bn3 = (\n",
    "            nn.BatchNorm2d(nlayer),\n",
    "            nn.BatchNorm2d(2*nlayer),\n",
    "            nn.BatchNorm2d(4*nlayer),\n",
    "        )\n",
    "        self.act = nn.LeakyReLU(0.1)\n",
    "        self.dropc = nn.Dropout(drop_conv)\n",
    "        self.dropf = nn.Dropout(drop_fc)\n",
    "        self.fc1 = nn.Linear((4*nlayer)*18 + 668, nfc)\n",
    "        self.fc2 = nn.Linear(nfc, 1)\n",
    "\n",
    "    def forward(self, pm4760):\n",
    "        x_mppc, x_pmt = torch.split(pm4760, [4092, 668], dim=1)\n",
    "        x_mppc = x_mppc.view(-1, 1, 93, 44)\n",
    "        x_mppc = self.act(self.dropc(self.bn1(self.conv1(x_mppc))))\n",
    "        x_mppc = self.act(self.dropc(self.bn2(self.conv2(x_mppc))))\n",
    "        x_mppc = self.act(self.dropc(self.bn3(self.conv3(x_mppc))))\n",
    "        x_mppc = x_mppc.view(x_mppc.size(0), -1)\n",
    "        z = torch.cat([x_mppc, x_pmt], dim=1)\n",
    "        z = self.act(self.dropf(self.fc1(z)))\n",
    "        out = self.fc2(z)        # linear output (predicts z)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  Training / Validation (streamed)\n",
    "# ----------------------------------------------------------------------\n",
    "def run_epoch_stream(\n",
    "    model, optimizer, device, root, tree,\n",
    "    step_size=4000, batch_size=128, train=True, amp=True,\n",
    "    rebalance=\"none\", edges=None, weights=None, per_bin_cap=0, max_chunks=None,\n",
    "    std_target=True, mu=0.0, sigma=1.0\n",
    "):\n",
    "    model.train(train)\n",
    "    loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n",
    "    device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    scaler = torch.amp.GradScaler(device_type, enabled=(amp and device_type == \"cuda\"))\n",
    "    total_loss, nobs = 0.0, 0\n",
    "    all_pred, all_true = [], []\n",
    "    NphoScale = 2e5\n",
    "\n",
    "    chunks_done = 0\n",
    "    for arr in iterate_chunks(root, tree, [\"Npho\", \"energy\"], step_size):\n",
    "        if max_chunks and chunks_done >= max_chunks:\n",
    "            break\n",
    "        chunks_done += 1\n",
    "\n",
    "        PM = np.log1p(arr[\"Npho\"].astype(\"float32\") / NphoScale)\n",
    "        E = arr[\"energy\"].astype(\"float32\").reshape(-1, 1)\n",
    "        Ez = (E - mu) / sigma if std_target else E\n",
    "\n",
    "        # --- balanced resampling per chunk\n",
    "        if rebalance == \"resample\" and edges is not None:\n",
    "            eflat = E.ravel()\n",
    "            bin_ids = np.clip(np.digitize(eflat, edges) - 1, 0, len(edges)-2)\n",
    "            idxs_by_bin = [np.where(bin_ids == i)[0] for i in range(len(edges)-1)]\n",
    "            sizes = [len(ix) for ix in idxs_by_bin if len(ix)]\n",
    "            if sizes:\n",
    "                take = per_bin_cap or min(sizes)\n",
    "                sel = np.concatenate([\n",
    "                    np.random.choice(ix, take, replace=False)\n",
    "                    for ix in idxs_by_bin if len(ix) >= take\n",
    "                ])\n",
    "                PM, E, Ez = PM[sel], E[sel], Ez[sel]\n",
    "\n",
    "        # --- dataset (optionally with weights)\n",
    "        if rebalance == \"loss\" and edges is not None and weights is not None:\n",
    "            eflat = E.ravel()\n",
    "            bin_ids = np.clip(np.digitize(eflat, edges) - 1, 0, len(edges)-2)\n",
    "            w_np = weights[bin_ids].astype(\"float32\").reshape(-1, 1)\n",
    "            ds = TensorDataset(torch.from_numpy(Ez), torch.from_numpy(PM), torch.from_numpy(w_np))\n",
    "        else:\n",
    "            ds = TensorDataset(torch.from_numpy(Ez), torch.from_numpy(PM))\n",
    "        loader = DataLoader(ds, batch_size=batch_size, shuffle=train, drop_last=False)\n",
    "\n",
    "        # --- loop over mini-batches\n",
    "        for batch in loader:\n",
    "            if rebalance == \"loss\":\n",
    "                Ez_b, PM_b, W_b = batch\n",
    "                W_b = W_b.to(device)\n",
    "            else:\n",
    "                Ez_b, PM_b = batch\n",
    "                W_b = None\n",
    "            Ez_b, PM_b = Ez_b.to(device), PM_b.to(device)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with torch.amp.autocast(device_type, enabled=(amp and device_type == \"cuda\"), dtype=torch.bfloat16):\n",
    "                    pred_z = model(PM_b)\n",
    "                    lvec = loss_fn(pred_z, Ez_b)\n",
    "                    loss = (lvec * W_b).mean() if W_b is not None else lvec.mean()\n",
    "                if amp and device_type == \"cuda\":\n",
    "                    scaler.scale(loss).backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.step(optimizer); scaler.update()\n",
    "                else:\n",
    "                    loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    pred_z = model(PM_b)\n",
    "                    lvec = loss_fn(pred_z, Ez_b)\n",
    "                    loss = (lvec * W_b).mean() if W_b is not None else lvec.mean()\n",
    "                    pred_e = (pred_z * sigma + mu).cpu().numpy()\n",
    "                    true_e = (Ez_b * sigma + mu).cpu().numpy()\n",
    "                    all_pred.append(pred_e); all_true.append(true_e)\n",
    "\n",
    "            total_loss += loss.item() * PM_b.size(0)\n",
    "            nobs += PM_b.size(0)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    loss_avg = total_loss / max(1, nobs)\n",
    "    if not train:\n",
    "        pred_np = np.concatenate(all_pred).ravel()\n",
    "        true_np = np.concatenate(all_true).ravel()\n",
    "        res = pred_np - true_np\n",
    "        return loss_avg, float(np.nanmean(res)), float(np.nanmean(np.abs(res)))\n",
    "    return loss_avg, float(\"nan\"), float(\"nan\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  Evaluation / Plot\n",
    "# ----------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def predict_stream(model, device, root, tree, step_size=4000, batch_size=256,\n",
    "                   std_target=True, mu=0.0, sigma=1.0):\n",
    "    model.eval()\n",
    "    preds, truths = [], []\n",
    "    for arr in iterate_chunks(root, tree, [\"Npho\", \"energy\"], step_size):\n",
    "        PM = np.log1p(arr[\"Npho\"].astype(\"float32\") / 2e5)\n",
    "        E = arr[\"energy\"].astype(\"float32\").reshape(-1, 1)\n",
    "        PM_t = torch.from_numpy(PM).to(device)\n",
    "        outs = []\n",
    "        for i in range(0, len(PM_t), batch_size):\n",
    "            z = model(PM_t[i:i+batch_size]).cpu().numpy()\n",
    "            outs.append((z * sigma + mu) if std_target else z)\n",
    "        preds.append(np.concatenate(outs)); truths.append(E)\n",
    "        torch.cuda.empty_cache()\n",
    "    return np.concatenate(preds).ravel(), np.concatenate(truths).ravel()\n",
    "\n",
    "\n",
    "def eval_plots(pred, true, outfile=None):\n",
    "    res = pred - true\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(res, bins=200)\n",
    "    plt.title(\"Residuals: Pred - True\"); \n",
    "    plt.xlabel(\"ΔE\"); \n",
    "    plt.ylabel(\"count\")\n",
    "    plt.tight_layout();\n",
    "    if outfile is None:\n",
    "        plt.show()\n",
    "    else: \n",
    "        plt.savefig(outfile, dpi=50)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  Main entry\n",
    "# ----------------------------------------------------------------------\n",
    "def main_with_args(\n",
    "    root, tree=\"tout\", epochs=10, batch=256, chunksize=4000, lr=3e-4,\n",
    "    weight_decay=1e-4, amp=True, rebalance=\"resample\",\n",
    "    bins=20, per_bin_cap=200, max_chunks=None, onnx=\"meg2enereg.onnx\",\n",
    "    mlflow_experiment=\"meg2_energy\", run_name=None\n",
    "):\n",
    "    root = os.path.expanduser(root)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = RegressorSimple().to(device)\n",
    "    with torch.no_grad(): \n",
    "        model.fc2.bias.fill_(1.374)\n",
    "\n",
    "    # optimizer (no decay for bias / BN)\n",
    "    decay, no_decay = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad: \n",
    "            continue\n",
    "        (no_decay if n.endswith(\".bias\") or \"bn\" in n.lower() else decay).append(p)\n",
    "    opt = torch.optim.RMSprop(\n",
    "        [{\"params\": decay, \"weight_decay\": weight_decay},\n",
    "         {\"params\": no_decay, \"weight_decay\": 0.0}],\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    # --- stats + histogram\n",
    "    mu, sigma = scan_energy_stats(root, tree, step_size=chunksize)\n",
    "    print(f\"[info] target mean={mu:.6f}  std={sigma:.6f}\")\n",
    "    edges = weights = None\n",
    "    if rebalance in (\"loss\", \"resample\"):\n",
    "        print(f\"[info] scanning histogram (bins={bins}) ...\")\n",
    "        edges, counts, weights = scan_energy_hist(root, tree, nbins=bins, step_size=chunksize)\n",
    "        print(f\"[info] non-empty bins {np.sum(counts>0)}/{bins}\")\n",
    "\n",
    "    # --- MLflow + TensorBoard setup\n",
    "    mlflow.set_experiment(mlflow_experiment)\n",
    "    if run_name is None:\n",
    "        run_name = time.strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "    writer = SummaryWriter(log_dir=os.path.join(\"runs\", run_name))\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_params({\n",
    "            \"root\": root,\n",
    "            \"tree\": tree,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch\": batch,\n",
    "            \"chunksize\": chunksize,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"amp\": amp,\n",
    "            \"rebalance\": rebalance,\n",
    "            \"bins\": bins,\n",
    "            \"per_bin_cap\": per_bin_cap,\n",
    "            \"max_chunks\": max_chunks,\n",
    "            \"model_nlayer\": 16,\n",
    "            \"model_nfc\": 128,\n",
    "        })\n",
    "        mlflow.log_metrics({\n",
    "            \"target_mean\": mu,\n",
    "            \"target_std\": sigma,\n",
    "        })\n",
    "\n",
    "        best_val, best_state = float(\"inf\"), None\n",
    "\n",
    "        # --- training\n",
    "        for ep in range(1, epochs+1):\n",
    "            t0 = time.time()\n",
    "            tr_loss, _, _ = run_epoch_stream(\n",
    "                model, opt, device, root, tree,\n",
    "                step_size=chunksize, batch_size=batch,\n",
    "                train=True, amp=amp,\n",
    "                rebalance=rebalance, edges=edges, weights=weights,\n",
    "                per_bin_cap=per_bin_cap, max_chunks=max_chunks,\n",
    "                std_target=True, mu=mu, sigma=sigma\n",
    "            )\n",
    "            val_loss, res_mean, mae = run_epoch_stream(\n",
    "                model, opt, device, root, tree,\n",
    "                step_size=chunksize, batch_size=max(batch,256),\n",
    "                train=False, amp=False,\n",
    "                rebalance=rebalance, edges=edges, weights=weights,\n",
    "                per_bin_cap=per_bin_cap, max_chunks=max_chunks,\n",
    "                std_target=True, mu=mu, sigma=sigma\n",
    "            )\n",
    "            sec = time.time() - t0\n",
    "            print(f\"[{ep:03d}] train {tr_loss:.6f}  val {val_loss:.6f}  \"\n",
    "                  f\"res_mean {res_mean:+.6f}  MAE {mae:.6f}  time {sec:.1f}s\")\n",
    "    \n",
    "            # --- log to MLflow per epoch\n",
    "            mlflow.log_metrics({\n",
    "                \"train_loss\": tr_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"res_mean\": res_mean,\n",
    "                \"mae\": mae,\n",
    "                \"epoch_time_sec\": sec,\n",
    "            }, step=ep)\n",
    "    \n",
    "            # --- log to TensorBoard\n",
    "            writer.add_scalar(\"loss/train\", tr_loss, ep)\n",
    "            writer.add_scalar(\"loss/val\", val_loss, ep)\n",
    "            writer.add_scalar(\"metric/mae\", mae, ep)\n",
    "            writer.add_scalar(\"metric/res_mean\", res_mean, ep)\n",
    "            writer.add_scalar(\"time/epoch_sec\", sec, ep)\n",
    "            \n",
    "            if val_loss < best_val:\n",
    "                best_val, best_state = val_loss, {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "    \n",
    "        # --- evaluation\n",
    "        if best_state: \n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        pred, true = predict_stream(\n",
    "            model, device, root, tree,\n",
    "            step_size=chunksize, batch_size=256,\n",
    "            std_target=True, mu=mu, sigma=sigma\n",
    "        )\n",
    "    \n",
    "        # save residual plot\n",
    "        residual_png = f\"residuals_{run_name}.png\"\n",
    "        eval_plots(pred, true, outfile=residual_png)\n",
    "        mlflow.log_artifact(residual_png)\n",
    "    \n",
    "        # --- export ONNX and log model\n",
    "        model.eval()\n",
    "        dummy = torch.randn(1, 4760, device=device)\n",
    "        torch.onnx.export(model, dummy, onnx,\n",
    "                          input_names=[\"Npho4760\"], output_names=[\"energy_z\"])\n",
    "        print(f\"[OK] Exported ONNX to {onnx}\")\n",
    "    \n",
    "        # log ONNX file + PyTorch model to MLflow\n",
    "        if os.path.exists(onnx):\n",
    "            mlflow.log_artifact(onnx)\n",
    "        mlflow.pytorch.log_model(model, \"pytorch_model\")\n",
    "    \n",
    "        writer.close()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  Example quick test\n",
    "# ----------------------------------------------------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     main_with_args(\n",
    "#         root=\"~/meghome/xec-ml-wl/CWMC.root\",\n",
    "#         tree=\"tout\",\n",
    "#         epochs=10,\n",
    "#         batch=256,\n",
    "#         chunksize=4000,\n",
    "#         lr=3e-4,\n",
    "#         amp=True,\n",
    "#         rebalance=\"resample\",   # or \"none\" / \"loss\"\n",
    "#         bins=20,\n",
    "#         per_bin_cap=200,\n",
    "#         max_chunks=3\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43b108c-ff9e-4272-831c-52cbfa6373b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/ext-li_w1/.conda/envs/xec-ml-wl/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2025/11/17 17:20:50 INFO mlflow.tracking.fluent: Experiment with name 'past_study_CW_energy' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] target mean=0.806783  std=0.113780\n",
      "[info] scanning histogram (bins=20) ...\n",
      "[info] non-empty bins 20/20\n",
      "[001] train 1.077576  val 1.108456  res_mean +0.181385  MAE 0.181385  time 34.2s\n",
      "[002] train 1.044638  val 1.061611  res_mean +0.175543  MAE 0.175543  time 17.4s\n",
      "[003] train 1.020584  val 1.019318  res_mean +0.170198  MAE 0.170198  time 17.4s\n",
      "[004] train 0.931840  val 0.969394  res_mean +0.163768  MAE 0.163768  time 17.3s\n",
      "[005] train 0.876044  val 0.881293  res_mean +0.152049  MAE 0.152049  time 17.3s\n",
      "[006] train 0.831680  val 0.815999  res_mean +0.143030  MAE 0.143030  time 17.4s\n",
      "[007] train 0.815546  val 0.802568  res_mean +0.141141  MAE 0.141141  time 17.3s\n",
      "[008] train 0.798796  val 0.797630  res_mean +0.140407  MAE 0.140407  time 17.4s\n",
      "[009] train 0.779845  val 0.792326  res_mean +0.139672  MAE 0.139672  time 17.4s\n",
      "[010] train 0.761143  val 0.737944  res_mean +0.131784  MAE 0.131787  time 17.5s\n",
      "[011] train 0.737634  val 0.752550  res_mean +0.133900  MAE 0.133901  time 17.3s\n",
      "[012] train 0.717542  val 0.702915  res_mean +0.126483  MAE 0.126498  time 17.4s\n",
      "[013] train 0.691226  val 0.707294  res_mean +0.127172  MAE 0.127184  time 17.4s\n",
      "[014] train 0.671263  val 0.655221  res_mean +0.119052  MAE 0.119075  time 17.4s\n",
      "[015] train 0.647927  val 0.609280  res_mean +0.111625  MAE 0.112516  time 18.7s\n",
      "[016] train 0.626411  val 0.595920  res_mean +0.109400  MAE 0.110965  time 17.7s\n",
      "[017] train 0.602219  val 0.613942  res_mean +0.112391  MAE 0.113038  time 17.4s\n",
      "[018] train 0.584043  val 0.610548  res_mean +0.111814  MAE 0.112656  time 17.3s\n",
      "[019] train 0.557265  val 0.554711  res_mean +0.102248  MAE 0.106024  time 17.4s\n",
      "[020] train 0.542165  val 0.583433  res_mean +0.107291  MAE 0.109459  time 17.5s\n",
      "[021] train 0.520371  val 0.563704  res_mean +0.103735  MAE 0.107057  time 17.4s\n",
      "[022] train 0.499016  val 0.510903  res_mean +0.094273  MAE 0.100582  time 17.3s\n",
      "[023] train 0.481875  val 0.466816  res_mean +0.085507  MAE 0.095135  time 17.4s\n",
      "[024] train 0.464931  val 0.483607  res_mean +0.088912  MAE 0.097257  time 17.4s\n",
      "[025] train 0.446094  val 0.446379  res_mean +0.081234  MAE 0.092691  time 17.4s\n",
      "[026] train 0.430785  val 0.471504  res_mean +0.086428  MAE 0.095742  time 17.3s\n",
      "[027] train 0.413947  val 0.447151  res_mean +0.081447  MAE 0.092799  time 17.5s\n",
      "[028] train 0.398116  val 0.420704  res_mean +0.075610  MAE 0.089591  time 17.4s\n",
      "[029] train 0.381021  val 0.387246  res_mean +0.067742  MAE 0.085593  time 17.4s\n",
      "[030] train 0.370477  val 0.387700  res_mean +0.067881  MAE 0.085632  time 17.2s\n",
      "[031] train 0.356671  val 0.441104  res_mean +0.080134  MAE 0.091966  time 17.4s\n",
      "[032] train 0.343752  val 0.351734  res_mean +0.058565  MAE 0.081582  time 17.4s\n",
      "[033] train 0.332647  val 0.438657  res_mean +0.079627  MAE 0.091666  time 17.4s\n",
      "[034] train 0.324159  val 0.368569  res_mean +0.062988  MAE 0.083511  time 17.3s\n",
      "[035] train 0.312272  val 0.397158  res_mean +0.070224  MAE 0.086721  time 17.4s\n",
      "[036] train 0.302109  val 0.380706  res_mean +0.066143  MAE 0.084849  time 17.4s\n",
      "[037] train 0.294999  val 0.404341  res_mean +0.071945  MAE 0.087626  time 17.3s\n",
      "[038] train 0.287969  val 0.302206  res_mean +0.042522  MAE 0.076236  time 17.4s\n",
      "[039] train 0.279710  val 0.294327  res_mean +0.039538  MAE 0.075293  time 17.5s\n",
      "[040] train 0.274974  val 0.346136  res_mean +0.056821  MAE 0.081013  time 18.1s\n",
      "[041] train 0.276284  val 0.292301  res_mean +0.038440  MAE 0.075089  time 17.3s\n",
      "[042] train 0.265753  val 0.298112  res_mean +0.040764  MAE 0.075813  time 17.4s\n",
      "[043] train 0.272388  val 0.280202  res_mean +0.033216  MAE 0.073704  time 17.4s\n",
      "[044] train 0.262940  val 0.262025  res_mean +0.022854  MAE 0.071758  time 17.4s\n",
      "[045] train 0.260655  val 0.330147  res_mean +0.052203  MAE 0.079210  time 17.3s\n",
      "[046] train 0.259917  val 0.351006  res_mean +0.058241  MAE 0.081531  time 17.4s\n",
      "[047] train 0.259092  val 0.281909  res_mean +0.033763  MAE 0.074030  time 17.4s\n",
      "[048] train 0.256724  val 0.298840  res_mean +0.041304  MAE 0.075858  time 17.4s\n",
      "[049] train 0.256782  val 0.334958  res_mean +0.053560  MAE 0.079760  time 17.4s\n",
      "[050] train 0.255201  val 0.293888  res_mean +0.039228  MAE 0.075302  time 17.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 17:35:53 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Exported ONNX to meg2enereg.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 17:35:54 WARNING mlflow.utils.requirements_utils: Found torch version (2.4.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.4.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/11/17 17:36:17 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.19.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torchvision==0.19.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/11/17 17:36:17 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "main_with_args(\n",
    "    root=\"~/meghome/xec-ml-wl/CWMC.root\",\n",
    "    tree=\"tout\",\n",
    "    epochs=50,\n",
    "    batch=256,\n",
    "    chunksize=4000,\n",
    "    lr=2e-4,\n",
    "    amp=True,\n",
    "    rebalance=\"resample\",   # or \"none\" / \"loss\"\n",
    "    bins=20,\n",
    "    per_bin_cap=200,\n",
    "    max_chunks=4000,\n",
    "    mlflow_experiment=\"past_study_CW_energy\",\n",
    "    run_name=\"energy_resample_2e-4_50ep\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8614a-9565-42ad-9d48-61bb6d5b7d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xec-ml-wl)",
   "language": "python",
   "name": "xec-ml-wl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
